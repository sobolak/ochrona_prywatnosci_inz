# -*- coding: utf-8 -*-
"""federated_differential_eminst.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lWq3cAU_Qcn6Xr_rWRQB3ugMv8e3Kl7u

# Prywatność różnicowa i uczenie federcyjne MINST

## Przygotowanie środowiska
"""

!pip install --quiet --upgrade tensorflow-federated-nightly
!pip install --quiet --upgrade nest-asyncio
!pip install --quiet --upgrade tb-nightly  

import nest_asyncio
import collections
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_federated as tff
import tensorflow_privacy as tfp
from matplotlib import pyplot as plt

nest_asyncio.apply()

@tff.federated_computation
def start():
  return 'Federated learning + differential privacy'

start()

"""## Przygotowanie danych"""

train, test = tff.simulation.datasets.emnist.load_data(
      only_digits=True)

print("Typ struktury") 
train.element_type_structure

print("Liczba użytkowników w zbiorze treningowym") 
len(train.client_ids)

print("Liczba użytkowników w zbiorze treningowym") 
len(test.client_ids)

figure_1 = plt.figure(figsize=(12, 7))
figure_1.suptitle('Ilość danych pewnego rodzaju dla serwera')
for i in range(20):
  first_clients_data = train.create_tf_dataset_for_client(train.client_ids[i])
  plot = collections.defaultdict(list)
  for example in first_clients_data:
    label = example['label'].numpy()
    plot[label].append(label)
  plt.subplot(4, 5, i+1)
  plt.title('Użytkownik {}'.format(i))
  for number in range(10):
    plt.hist(
        plot[number],
        density=False,
        bins=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

all_statistics = [[],[],[],[],[],[],[],[],[],[]]
clients_sum = []
for i in range(len(test.client_ids)):
  dataset = train.create_tf_dataset_for_client(train.client_ids[i])
  clients_sum.append(0)
  if (i % 750 == 0):
    print("Użytkownik: " + str(i))
  for j in range(10):
    all_statistics[j].append(0)
  for example in dataset:
    label = example['label'].numpy()
    all_statistics[label][i] += 1
    clients_sum[i] += 1

from numpy import std,mean
print("Średnia ilość próbek " + str(mean(clients_sum)))
print("Średnie ochylenie standardowe ilości próbek  " + str(std(clients_sum)))

figure_2 = plt.figure()
figure_2.suptitle('Number of samples from one client')

plt.plot(clients_sum, list(range(0, len(train.client_ids))))
plt.show()

from numpy.core.fromnumeric import mean

plot_statistics_list = [[],[],[]]
bins = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
for i in range(10):
  plot_statistics_list[0].append(mean(all_statistics[i]))
  plot_statistics_list[1].append(max(all_statistics[i]))
  plot_statistics_list[2].append(min(all_statistics[i]))

figure_3 = plt.figure()
figure_3.suptitle('Statystyki ilościowe danych')

plt.xlim(0, 9)
plt.plot(bins, plot_statistics_list[0], label = "średnia")
plt.plot(bins, plot_statistics_list[1], label = "max")
plt.plot(bins, plot_statistics_list[2], label = "min")
plt.legend()
plt.show()

def get_emnist_dataset():
  emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data(
      only_digits=True)

  def element_change(element):
    return collections.OrderedDict(
        x=tf.expand_dims(element['pixels'], -1), y=element['label'])

  def train_preprocess(dataset):
    return (dataset.map(element_change)
                   .shuffle(buffer_size=418)
                   .repeat(1)
                   .batch(32, drop_remainder=False))

  def test_preprocess(dataset):
    return dataset.map(element_change).batch(128, drop_remainder=False)

  emnist_train = emnist_train.preprocess(train_preprocess)
  emnist_test = test_preprocess(
      emnist_test.create_tf_dataset_from_all_clients())
  return emnist_train, emnist_test

emnist_train, emnist_test = get_emnist_dataset()

def my_model_fn():
  model = tf.keras.models.Sequential([
      tf.keras.layers.Reshape(input_shape=(28, 28, 1), target_shape=(28 * 28,)),
      tf.keras.layers.Dense(200, activation=tf.nn.relu),
      tf.keras.layers.Dense(200, activation=tf.nn.relu),
      tf.keras.layers.Dense(10)])
  return tff.learning.from_keras_model(
      keras_model=model,
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      input_spec=emnist_test.element_spec,
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

tff.backends.native.set_local_python_execution_context(clients_per_thread=10)

total_clients = len(emnist_train.client_ids)

def train(rounds, noise_multiplier, clients_per_round, data_frame):
  aggregation_factory = tff.learning.model_update_aggregator.dp_aggregator(
      noise_multiplier, clients_per_round)

  sampling_prob = clients_per_round / total_clients

  learning_process = tff.learning.algorithms.build_unweighted_fed_avg(
        my_model_fn,
        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.01),
        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0, momentum=0.9),
        model_aggregator=aggregation_factory)

  eval_process = tff.learning.build_federated_evaluation(my_model_fn)

  state = learning_process.initialize()
  for round in range(rounds):
    if round % 2 == 0:
      model_weights = learning_process.get_model_weights(state)
      metrics = eval_process(model_weights, [emnist_test])['eval']
      print(f'Runda {round:3d}: {metrics}')
      data_frame = data_frame.append({'Runda': round,
                                      'NoiseMultiplier': noise_multiplier,
                                      **metrics}, ignore_index=True)
    x = np.random.uniform(size=total_clients)
    sampled_clients = [
        emnist_train.client_ids[i] for i in range(total_clients)
        if x[i] < sampling_prob]
    sampled_train_data = [
        emnist_train.create_tf_dataset_for_client(client)
        for client in sampled_clients]

    result = learning_process.next(state, sampled_train_data)
    state = result.state
    metrics = result.metrics

  model_weights = learning_process.get_model_weights(state)
  metrics = eval_process(model_weights, [emnist_test])['eval']
  print(f'Runda {rounds:3d}: {metrics}')
  data_frame = data_frame.append({'Runda': rounds,
                                  'NoiseMultiplier': noise_multiplier,
                                  **metrics}, ignore_index=True)

  return data_frame

"""Proces testowania

"""

def client10(noise_multiplier):
  rounds = 2
  clients_per_round = 10
  data_noise_10_clients_basic_model = pd.DataFrame()

  print("Mnożnik szumu " + str(noise_multiplier))

  data_noise_10_clients_basic_model = train(rounds, noise_multiplier, clients_per_round, data_noise_10_clients_basic_model)
  print()

  return data_noise_10_clients_basic_model

buff = client10(0.0)

buff = client10(0.25)

buff = client10(0.5)

buff = client10(0.75)

def client30(noise_multiplier):
  data_noise_30_clients_basic_model = pd.DataFrame()
  rounds = 2
  clients_per_round = 30

  print("Mnożnik szumu " + str(noise_multiplier))

  data_noise_30_clients_basic_model = train(rounds, noise_multiplier, clients_per_round, data_noise_30_clients_basic_model)
  print()

  return data_noise_30_clients_basic_model

buff = client30(0.0)

buff = client30(0.25)

buff = client30(0.5)

buff = client30(0.75)

buff = client30(1.0)

def client50(noise_multiplier):
  data_noise_50_clients_basic_model = pd.DataFrame()
  rounds = 2
  clients_per_round = 50

  print("Mnożnik szumu " + str(noise_multiplier))
  data_noise_50_clients_basic_model = train(rounds, noise_multiplier, clients_per_round, data_noise_50_clients_basic_model)
  print()

  return data_noise_50_clients_basic_model

buff = client50(0.0)

buff = client50(0.25)

buff = client50(0.5)

buff = client50(0.75)

buff = client50(1.0)

def client70(noise_multiplier):
  data_noise_70_clients_basic_model = pd.DataFrame()
  rounds = 2
  clients_per_round = 70

  print("Mnożnik szumu " + str(noise_multiplier))
  data_noise_70_clients_basic_model = train(rounds, noise_multiplier, clients_per_round, data_noise_70_clients_basic_model)
  print()

  return data_noise_70_clients_basic_model

buff = client70(0.0)

buff = client70(0.25)

buff = client70(0.5)

buff = client70(0.75)

buff = client70(1.2)

buff = client70(1.4)

def client100(noise_multiplier):
  data_noise_100_clients_basic_model = pd.DataFrame()
  rounds = 2
  clients_per_round = 100

  print("Mnożnik szumu " + str(noise_multiplier))
  data_noise_100_clients_basic_model = train(rounds, noise_multiplier, clients_per_round, data_noise_100_clients_basic_model)
  print()

  return data_noise_100_clients_basic_model

buff = client100(0.0)

buff = client100(0.25)

buff = client100(0.5)

buff = client100(0.75)

buff = client100(1.0)

buff = client100(1.2)

buff = client100(1.4)

buff = client100(1.4)

buff = client100(1.6)

buff = client100(1.6)

buff = client100(1.4)

"""Wykorzystane źródła TensorFlow federated learning:

*   https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification
*   https://www.tensorflow.org/federated/tutorials/federated_learning_with_differential_privacy



"""